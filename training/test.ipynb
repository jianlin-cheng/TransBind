{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Import your model class from the training script\n",
    "from train import ProteinAware_TransBind  #\n",
    "\n",
    "# --- Step 1: Load the trained model ---\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"\"\n",
    "MAPPING_FILE = \"../data/tf_to_feature_mapping_exact.json\"\n",
    "FEATURES_DIR = \"../data/tf_features/\"\n",
    "\n",
    "print(\"Loading trained Protein-Aware model...\")\n",
    "model = ProteinAware_TransBind.load_from_checkpoint(\n",
    "    CHECKPOINT_PATH,\n",
    "    mapping_file=MAPPING_FILE,\n",
    "    features_dir=FEATURES_DIR\n",
    ")\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "print(f\"Using cross-attention: {model.use_cross_attention}\")\n",
    "print(f\"TF feature projection dimension: {model.tf_feature_projection_dim}\")\n",
    "\n",
    "# --- Step 2: Load test data ---\n",
    "print(\"Loading test data...\")\n",
    "with h5py.File('/bml/shreya/BenchMarking_TF/Bioinfor-DeepSEA/data/tfrecords/test.mat', 'r') as testmat:\n",
    "    X_test = np.array(testmat['testxdata'])  # Shape: (N, 1000, 4)\n",
    "    y_test = np.array(testmat['testdata'])[:, :690]  # Ensure 690 labels\n",
    "\n",
    "print(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensor and move to device\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "# --- Step 3: Make predictions ---\n",
    "print(\"Making predictions...\")\n",
    "batch_size = 100\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test_tensor), batch_size):\n",
    "        batch = X_test_tensor[i:i + batch_size]\n",
    "        # Get raw logits and apply sigmoid to get probabilities\n",
    "        batch_logits = model(batch)\n",
    "        batch_pred = torch.sigmoid(batch_logits).cpu().numpy()\n",
    "        preds.append(batch_pred)\n",
    "        \n",
    "        # Print progress\n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + len(batch)}/{len(X_test_tensor)} samples...\")\n",
    "\n",
    "pred_y = np.concatenate(preds, axis=0)\n",
    "print(f\"Predictions shape: {pred_y.shape}\")\n",
    "\n",
    "# --- Step 4: Evaluate metrics ---\n",
    "print(\"Calculating metrics...\")\n",
    "roc_scores, pr_scores = [], []\n",
    "\n",
    "# Create output filename with model info\n",
    "output_filename = f'aucs_protein_aware_{\"cross_attn\" if model.use_cross_attention else \"simple\"}.txt'\n",
    "\n",
    "with open(output_filename, 'w') as aucs_file:\n",
    "    aucs_file.write('Protein-Aware AU ROC\\tProtein-Aware AU PR\\n')\n",
    "    \n",
    "    for i in range(690):\n",
    "        try:\n",
    "            # Check if we have both positive and negative samples\n",
    "            if len(np.unique(y_test[:, i])) > 1:\n",
    "                roc_auc = roc_auc_score(y_test[:, i], pred_y[:, i])\n",
    "                pr_auc = average_precision_score(y_test[:, i], pred_y[:, i])\n",
    "                roc_scores.append(roc_auc)\n",
    "                pr_scores.append(pr_auc)\n",
    "                aucs_file.write(f'{roc_auc:.5f}\\t{pr_auc:.5f}\\n')\n",
    "            else:\n",
    "                # All samples are the same class (usually all negative)\n",
    "                roc_scores.append(np.nan)\n",
    "                pr_scores.append(np.nan)\n",
    "                aucs_file.write('NaN\\tNaN\\n')\n",
    "        except ValueError as e:\n",
    "            print(f\"Error calculating metrics for label {i}: {e}\")\n",
    "            roc_scores.append(np.nan)\n",
    "            pr_scores.append(np.nan)\n",
    "            aucs_file.write('NaN\\tNaN\\n')\n",
    "    \n",
    "    # Calculate and write averages\n",
    "    avg_roc = np.nanmean(roc_scores)\n",
    "    avg_pr = np.nanmean(pr_scores)\n",
    "    median_roc = np.nanmedian(roc_scores)\n",
    "    median_pr = np.nanmedian(pr_scores)\n",
    "    \n",
    "    aucs_file.write(f'\\nAVERAGE\\t{avg_roc:.5f}\\t{avg_pr:.5f}\\n')\n",
    "    aucs_file.write(f'MEDIAN\\t{median_roc:.5f}\\t{median_pr:.5f}\\n')\n",
    "    \n",
    "    # Count valid scores\n",
    "    valid_roc_count = np.sum(~np.isnan(roc_scores))\n",
    "    valid_pr_count = np.sum(~np.isnan(pr_scores))\n",
    "    aucs_file.write(f'VALID_SCORES\\t{valid_roc_count}/690\\t{valid_pr_count}/690\\n')\n",
    "\n",
    "# Print summary\n",
    "print('\\n' + '='*60)\n",
    "print('PROTEIN-AWARE PERFORMANCE SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Model configuration:')\n",
    "print(f'  - Cross-attention: {model.use_cross_attention}')\n",
    "print(f'  - TF feature projection dim: {model.tf_feature_projection_dim}')\n",
    "print(f'  - Using real protein features: True')\n",
    "print(f'')\n",
    "print(f'Results:')\n",
    "print(f'  - Average ROC AUC: {avg_roc:.5f}')\n",
    "print(f'  - Average PR AUC: {avg_pr:.5f}')\n",
    "print(f'  - Median ROC AUC: {median_roc:.5f}')\n",
    "print(f'  - Median PR AUC: {median_pr:.5f}')\n",
    "print(f'  - Valid ROC scores: {valid_roc_count}/690')\n",
    "print(f'  - Valid PR scores: {valid_pr_count}/690')\n",
    "print(f'')\n",
    "print(f'Results saved to: {output_filename}')\n",
    "print('='*60)\n",
    "\n",
    "# Additional analysis: Show distribution of scores\n",
    "print('\\nScore distribution:')\n",
    "print(f'ROC AUC - Min: {np.nanmin(roc_scores):.3f}, Max: {np.nanmax(roc_scores):.3f}')\n",
    "print(f'PR AUC - Min: {np.nanmin(pr_scores):.3f}, Max: {np.nanmax(pr_scores):.3f}')\n",
    "\n",
    "# Show top performing labels\n",
    "if valid_roc_count > 0:\n",
    "    top_indices = np.argsort(roc_scores)[-10:][::-1]  # Top 10 ROC scores\n",
    "    print(f'\\nTop 10 TF labels by ROC AUC:')\n",
    "    for idx in top_indices:\n",
    "        if not np.isnan(roc_scores[idx]):\n",
    "            print(f'  Label {idx}: ROC={roc_scores[idx]:.3f}, PR={pr_scores[idx]:.3f}')\n",
    "\n",
    "print('\\nTesting completed!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from train_general import GeneralizedProteinAware_TransBind\n",
    "\n",
    "def load_new_tf_features(fea_file_path):\n",
    "    \"\"\"Load .fea file containing protein features for new TF\"\"\"\n",
    "    with open(fea_file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    numbers = [float(x) for x in content.split()]\n",
    "    data = np.array(numbers)\n",
    "    \n",
    "    if len(numbers) % 1280 == 0:\n",
    "        sequence_length = len(numbers) // 1280\n",
    "        data_matrix = data.reshape(sequence_length, 1280)\n",
    "        feature_vector = np.mean(data_matrix, axis=0)\n",
    "        return torch.FloatTensor(feature_vector)\n",
    "    elif len(numbers) == 1280:\n",
    "        return torch.FloatTensor(data)\n",
    "    else:\n",
    "        if len(numbers) >= 1280:\n",
    "            return torch.FloatTensor(data[:1280])\n",
    "        else:\n",
    "            raise ValueError(f\"Feature file too small: {len(numbers)} < 1280\")\n",
    "\n",
    "def load_new_dna_data(mat_file_path):\n",
    "    \"\"\"Load DNA data from .mat file\"\"\"\n",
    "    mat_data = scipy.io.loadmat(mat_file_path)\n",
    "    dna_sequences = mat_data['testxdata']\n",
    "    labels = mat_data['testdata']\n",
    "    \n",
    "    if labels.ndim > 1:\n",
    "        labels = labels.flatten()\n",
    "    \n",
    "    return torch.FloatTensor(dna_sequences), torch.FloatTensor(labels)\n",
    "\n",
    "def plot_confusion_matrix(cm, threshold, save_path):\n",
    "    \"\"\"Plot and save confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    \n",
    "    plt.title(f'Confusion Matrix (Threshold: {threshold})')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_confusion_matrix(cm):\n",
    "    \"\"\"Analyze confusion matrix and return detailed metrics\"\"\"\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Basic metrics\n",
    "    total = tn + fp + fn + tp\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Additional metrics\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    positive_predictive_value = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    negative_predictive_value = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    balanced_accuracy = (recall + specificity) / 2\n",
    "    \n",
    "    return {\n",
    "        'true_negatives': int(tn),\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'true_positives': int(tp),\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'specificity': float(specificity),\n",
    "        'f1_score': float(f1),\n",
    "        'false_positive_rate': float(false_positive_rate),\n",
    "        'false_negative_rate': float(false_negative_rate),\n",
    "        'positive_predictive_value': float(positive_predictive_value),\n",
    "        'negative_predictive_value': float(negative_predictive_value),\n",
    "        'balanced_accuracy': float(balanced_accuracy)\n",
    "    }\n",
    "\n",
    "def evaluate_new_tf():\n",
    "    \"\"\"\n",
    "    Clean evaluation of new TF (GATA4) with comprehensive metrics including confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª EVALUATING NEW TF: GATA4 (Mouse)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # File paths\n",
    "\n",
    "    MODEL_PATH =\"\"\n",
    "    MAPPING_FILE = \".../data/tf_to_feature_mapping_exact.json\"\n",
    "    FEATURES_DIR = \".../data/tf_features/\"\n",
    "    NEW_TF_FEA_FILE = \".../data/fasta_human/output_fea/HNF1A.fea\"\n",
    "    NEW_DNA_MAT_FILE = \"/bml/shreya/TF_binding_site/dataset_test/DeepSEA_dataset/new_tf/HNF1A_test.mat\"\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading trained model...\")\n",
    "    model = GeneralizedProteinAware_TransBind.load_from_checkpoint(\n",
    "        MODEL_PATH, mapping_file=MAPPING_FILE, features_dir=FEATURES_DIR\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading HNF1A features and DNA sequences...\")\n",
    "    new_tf_features = load_new_tf_features(NEW_TF_FEA_FILE).to(device)\n",
    "    dna_sequences, true_labels = load_new_dna_data(NEW_DNA_MAT_FILE)\n",
    "    \n",
    "    print(f\"Dataset: {len(dna_sequences):,} sequences\")\n",
    "    print(f\"Positive samples: {int(true_labels.sum()):,} ({true_labels.mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Make predictions on subset (50,000 sequences for quick testing)\n",
    "    n_test = min(1500000, len(dna_sequences))\n",
    "    print(f\"Making predictions on {n_test:,} sequences...\")\n",
    "    \n",
    "    predictions = []\n",
    "    batch_size = 5000\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_test, batch_size):\n",
    "            end_idx = min(i + batch_size, n_test)\n",
    "            batch_predictions = []\n",
    "            \n",
    "            for j in range(i, end_idx):\n",
    "                single_dna = dna_sequences[j:j+1].to(device)\n",
    "                binding_prob, _ = model.predict_new_tf(single_dna, new_tf_features)\n",
    "                batch_predictions.append(binding_prob.item())\n",
    "            \n",
    "            predictions.extend(batch_predictions)\n",
    "            \n",
    "            if (end_idx) % 5000 == 0:\n",
    "                print(f\"  Processed {end_idx:,}/{n_test:,}\")\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = true_labels[:n_test].numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nðŸ“Š PERFORMANCE METRICS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # ROC AUC and PR AUC\n",
    "    auroc = roc_auc_score(true_labels, predictions)\n",
    "    aupr = average_precision_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"ROC AUC:     {auroc:.4f}\")\n",
    "    print(f\"PR AUC:      {aupr:.4f}\")\n",
    "    \n",
    "    # Test different thresholds for binary classification\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    best_mcc = -1\n",
    "    best_threshold = 0.5\n",
    "    confusion_matrices = {}\n",
    "    \n",
    "    print(f\"\\nTHRESHOLD ANALYSIS:\")\n",
    "    print(\"Thresh  Acc    Prec   Rec    F1     MCC    Spec   BAcc\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        pred_binary = (predictions >= thresh).astype(int)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(true_labels, pred_binary)\n",
    "        confusion_matrices[thresh] = cm\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(true_labels, pred_binary)\n",
    "        \n",
    "        # Handle cases where precision/recall might be undefined\n",
    "        try:\n",
    "            prec = precision_score(true_labels, pred_binary, zero_division=0)\n",
    "            rec = recall_score(true_labels, pred_binary, zero_division=0)\n",
    "            f1 = f1_score(true_labels, pred_binary, zero_division=0)\n",
    "            mcc = matthews_corrcoef(true_labels, pred_binary)\n",
    "            \n",
    "            # Calculate specificity and balanced accuracy from confusion matrix\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            balanced_acc = (rec + spec) / 2\n",
    "            \n",
    "        except:\n",
    "            prec = rec = f1 = mcc = spec = balanced_acc = 0.0\n",
    "        \n",
    "        print(f\"{thresh:5.1f}  {acc:5.3f}  {prec:5.3f}  {rec:5.3f}  {f1:5.3f}  {mcc:6.3f}  {spec:5.3f}  {balanced_acc:5.3f}\")\n",
    "        \n",
    "        # Track best MCC\n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_threshold = thresh\n",
    "    \n",
    "    print(f\"\\nBest threshold: {best_threshold} (MCC: {best_mcc:.4f})\")\n",
    "    \n",
    "    # Final metrics at best threshold\n",
    "    pred_binary_best = (predictions >= best_threshold).astype(int)\n",
    "    best_cm = confusion_matrix(true_labels, pred_binary_best)\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs('./results_new/', exist_ok=True)\n",
    "    \n",
    "    # Plot confusion matrix for best threshold\n",
    "    plot_confusion_matrix(best_cm, best_threshold, './results_new/confusion_matrix_best.png')\n",
    "    \n",
    "    # Analyze confusion matrix\n",
    "    cm_analysis = analyze_confusion_matrix(best_cm)\n",
    "    \n",
    "    print(f\"\\nðŸ” CONFUSION MATRIX ANALYSIS (threshold={best_threshold}):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"True Negatives (TN):   {cm_analysis['true_negatives']:,}\")\n",
    "    print(f\"False Positives (FP):  {cm_analysis['false_positives']:,}\")\n",
    "    print(f\"False Negatives (FN):  {cm_analysis['false_negatives']:,}\")\n",
    "    print(f\"True Positives (TP):   {cm_analysis['true_positives']:,}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Sensitivity (Recall):  {cm_analysis['recall']:.4f}\")\n",
    "    print(f\"Specificity:           {cm_analysis['specificity']:.4f}\")\n",
    "    print(f\"Precision (PPV):       {cm_analysis['precision']:.4f}\")\n",
    "    print(f\"Negative Pred Value:   {cm_analysis['negative_predictive_value']:.4f}\")\n",
    "    print(f\"False Positive Rate:   {cm_analysis['false_positive_rate']:.4f}\")\n",
    "    print(f\"False Negative Rate:   {cm_analysis['false_negative_rate']:.4f}\")\n",
    "    print(f\"Balanced Accuracy:     {cm_analysis['balanced_accuracy']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ FINAL METRICS (threshold={best_threshold}):\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"Accuracy:    {accuracy_score(true_labels, pred_binary_best):.4f}\")\n",
    "    print(f\"Precision:   {precision_score(true_labels, pred_binary_best, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:      {recall_score(true_labels, pred_binary_best, zero_division=0):.4f}\")\n",
    "    print(f\"F1-Score:    {f1_score(true_labels, pred_binary_best, zero_division=0):.4f}\")\n",
    "    print(f\"MCC:         {matthews_corrcoef(true_labels, pred_binary_best):.4f}\")\n",
    "    print(f\"ROC AUC:     {auroc:.4f}\")\n",
    "    print(f\"PR AUC:      {aupr:.4f}\")\n",
    "    \n",
    "    # Check if inverted predictions perform better\n",
    "    print(f\"\\nðŸ”„ INVERTED PREDICTIONS CHECK:\")\n",
    "    inverted_predictions = 1 - predictions\n",
    "    auroc_inv = roc_auc_score(true_labels, inverted_predictions)\n",
    "    aupr_inv = average_precision_score(true_labels, inverted_predictions)\n",
    "    \n",
    "    print(f\"ROC AUC (inverted):  {auroc_inv:.4f}\")\n",
    "    print(f\"PR AUC (inverted):   {aupr_inv:.4f}\")\n",
    "    \n",
    "    if auroc_inv > auroc:\n",
    "        print(\"âš ï¸  Inverted predictions perform better!\")\n",
    "        print(\"   This suggests competitive binding relationship\")\n",
    "        better_auroc = auroc_inv\n",
    "        better_aupr = aupr_inv\n",
    "        relationship = \"competitive\"\n",
    "    else:\n",
    "        print(\"âœ… Original predictions are better\")\n",
    "        better_auroc = auroc\n",
    "        better_aupr = aupr\n",
    "        relationship = \"cooperative\"\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nðŸ“ˆ PREDICTION STATISTICS:\")\n",
    "    print(f\"Mean:        {predictions.mean():.4f}\")\n",
    "    print(f\"Median:      {np.median(predictions):.4f}\")\n",
    "    print(f\"Std:         {predictions.std():.4f}\")\n",
    "    print(f\"Min:         {predictions.min():.4f}\")\n",
    "    print(f\"Max:         {predictions.max():.4f}\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    print(f\"\\nPREDICTION DISTRIBUTION:\")\n",
    "    for thresh in [0.1, 0.2, 0.3, 0.5, 0.7]:\n",
    "        count = np.sum(predictions > thresh)\n",
    "        pct = count / len(predictions) * 100\n",
    "        print(f\"  > {thresh:3.1f}: {count:6d} ({pct:4.1f}%)\")\n",
    "    \n",
    "    # Save results_new\n",
    "    results_new = {\n",
    "        'tf_name': 'HNF1A_mouse',\n",
    "        'n_samples': len(predictions),\n",
    "        'n_positive': int(np.sum(true_labels)),\n",
    "        'positive_rate': float(np.mean(true_labels)),\n",
    "        'auroc': float(auroc),\n",
    "        'aupr': float(aupr),\n",
    "        'auroc_inverted': float(auroc_inv),\n",
    "        'aupr_inverted': float(aupr_inv),\n",
    "        'best_threshold': float(best_threshold),\n",
    "        'best_mcc': float(best_mcc),\n",
    "        'accuracy': float(accuracy_score(true_labels, pred_binary_best)),\n",
    "        'precision': float(precision_score(true_labels, pred_binary_best, zero_division=0)),\n",
    "        'recall': float(recall_score(true_labels, pred_binary_best, zero_division=0)),\n",
    "        'f1_score': float(f1_score(true_labels, pred_binary_best, zero_division=0)),\n",
    "        'relationship': relationship,\n",
    "        'mean_prediction': float(predictions.mean()),\n",
    "        'std_prediction': float(predictions.std()),\n",
    "        'confusion_matrix_analysis': cm_analysis,\n",
    "        'confusion_matrix': best_cm.tolist()\n",
    "    }\n",
    "    \n",
    "    # Save confusion matrices for all thresholds\n",
    "    cm_results_new = {}\n",
    "    for thresh, cm in confusion_matrices.items():\n",
    "        cm_results_new[f'threshold_{thresh}'] = {\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'analysis': analyze_confusion_matrix(cm)\n",
    "        }\n",
    "        # Save individual confusion matrix plots\n",
    "        plot_confusion_matrix(cm, thresh, f'./results_new/confusion_matrix_thresh_{thresh}.png')\n",
    "    \n",
    "    # Save to files\n",
    "    import json\n",
    "    with open('./results_new/HNF1A_evaluation_results_new.json', 'w') as f:\n",
    "        json.dump(results_new, f, indent=2)\n",
    "    \n",
    "    with open('./results_new/HNF1A_confusion_matrices.json', 'w') as f:\n",
    "        json.dump(cm_results_new, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ results_new saved to: ./results_new/HNF1A_evaluation_results_new.json\")\n",
    "    print(f\"ðŸ’¾ Confusion matrices saved to: ./results_new/HNF1A_confusion_matrices.json\")\n",
    "    print(f\"ðŸ“Š Confusion matrix plots saved to: ./results_new/confusion_matrix_*.png\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"SUMMARY: HNF1A (Mouse) Evaluation\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Dataset Size:     {len(predictions):,} sequences\")\n",
    "    print(f\"Positive Rate:    {np.mean(true_labels)*100:.1f}%\")\n",
    "    print(f\"Best ROC AUC:     {better_auroc:.4f}\")\n",
    "    print(f\"Best PR AUC:      {better_aupr:.4f}\")\n",
    "    print(f\"Best MCC:         {best_mcc:.4f}\")\n",
    "    print(f\"Sensitivity:      {cm_analysis['recall']:.4f}\")\n",
    "    print(f\"Specificity:      {cm_analysis['specificity']:.4f}\")\n",
    "    print(f\"Balanced Acc:     {cm_analysis['balanced_accuracy']:.4f}\")\n",
    "    print(f\"Relationship:     {relationship.title()}\")\n",
    "    print(f\"Generalization:   âœ… SUCCESS\")\n",
    "    print(f\"=\"*50)\n",
    "    \n",
    "    return results_new\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = evaluate_new_tf()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
